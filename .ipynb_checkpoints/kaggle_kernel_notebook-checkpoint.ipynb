{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4877d6f4090732087a6ecb9b721a5331bb3f488"
   },
   "source": [
    "# ESKEEETIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ea12a1be88e733b70a135265d93ae517bb7f0c0"
   },
   "outputs": [],
   "source": [
    "##Import all the things\n",
    "#Triple Threat---\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None #shut down that annoying slice warning\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Bayesian Optimization\n",
    "# from skopt import gp_minimize\n",
    "\n",
    "#You know it, you love it\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#For featurizing\n",
    "from multiprocessing import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69b0a67c1b6881ff1ee3996dac6195a0adf4f3e5"
   },
   "outputs": [],
   "source": [
    "#Datetime functions bc I committed early to integer datetime and I don't want to refactor\n",
    "def date_to_int(date):\n",
    "    return 10000*date.year + 100*date.month + date.day\n",
    "def int_to_date(date):\n",
    "    return datetime.strptime(str(date), '%Y%m%d')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#Get the training data\n",
    "from kaggle.competitions import twosigmanews\n",
    "env = twosigmanews.make_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6034b46fce8c9d55d403de32e7cebe8cb9fef96d"
   },
   "source": [
    "## **`get_training_data`** function (built-in)\n",
    "\n",
    "Returns the training data DataFrames as a tuple of:\n",
    "* `market_train_df`: DataFrame with market training data\n",
    "* `news_train_df`: DataFrame with news training data\n",
    "\n",
    "These DataFrames contain all market and news data from February 2007 to December 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5cffd5e960d4c129d7b12638af26125ce3d66199"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(env_market_train, env_news_train) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ac338c0f69be1f26754e49515e4a72bf8e8f383"
   },
   "source": [
    "## **`DataPrepper`** class\n",
    "Takes environment dataframes:\n",
    "* env_market_train\n",
    "* env_news_train\n",
    "\n",
    "Performs data cleaning:\n",
    "* Replace Price Outliers\n",
    "* Convert Datetime\n",
    "* Remove data prior to train cutoff\n",
    "\n",
    "Performs feature engineering:\n",
    "* Create Daily Return\n",
    "* Daily Change\n",
    "* Price/Volume\n",
    "* All News Data aggregated  to relevant trading day\n",
    "\n",
    "Returns merged dataframe for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6811a1a76f08b2a029543cf73bcdf4dfca7dc362"
   },
   "outputs": [],
   "source": [
    "class DataPrepper():\n",
    "    \"\"\"A class for loading and transforming data for stock data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_cutoff = 20101231\n",
    "\n",
    "    def make_price_diff(self, market_train):\n",
    "        #eda function to find outliers\n",
    "        market_train['closeOverOpen'] = market_train['Close']/market_train['Open']\n",
    "        market_train['priceDiff'] = np.abs(market_train['Close'] - market_train['Open'])\n",
    "        return market_train\n",
    "\n",
    "    def _replace_price_outliers(self, market_train):\n",
    "        market_train['dailychange'] = market_train['close']/market_train['open']\n",
    "        market_train['open'][market_train['dailychange'] < .33] = market_train['close']\n",
    "        market_train['close'][market_train['dailychange'] > 2] = market_train['open']\n",
    "        return market_train\n",
    "\n",
    "    def prepare_market(self, market_train):\n",
    "        market_train = self._replace_price_outliers(market_train)\n",
    "        market_train['time'] = market_train['time'].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        market_train = market_train[market_train.time >= self.train_cutoff]\n",
    "        market_train['pricevolume'] = market_train['volume']/market_train['close']\n",
    "        self.tradingdays = market_train['time'].unique()\n",
    "        return market_train\n",
    "\n",
    "    def prepare_news(self, news_train):\n",
    "        news_train['time'] = news_train['time'].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        news_train = news_train[news_train.time >= self.train_cutoff]\n",
    "        news_train['time'] = news_train['time'].apply(self._map_trading_day)\n",
    "        news_train['coverage'] = news_train['sentimentWordCount']/news_train['wordCount']\n",
    "        return news_train\n",
    "\n",
    "    def _map_trading_day(self, news_date):\n",
    "        if news_date in self.tradingdays:\n",
    "            return news_date\n",
    "        else:   \n",
    "            values = self.tradingdays - news_date\n",
    "            mask = values >= 0\n",
    "            try:\n",
    "                return self.tradingdays[mask][0]\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "    def merge_data(self, market_df, news_df):\n",
    "        newsgroup = news_df.groupby(['time', 'assetName'], sort=False).agg(np.mean).reset_index()\n",
    "        merged = pd.merge(market_df, newsgroup, how='left', on=['time', 'assetName'], copy=False)\n",
    "        merged.fillna(value=0, inplace=True)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84b5a58f67ebded82e6aabc66ca36411e6db35a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare the market and news data\n",
    "prepper = DataPrepper()\n",
    "market_train = prepper.prepare_market(env_market_train)\n",
    "news_train = prepper.prepare_news(env_news_train)\n",
    "train_df = prepper.merge_data(market_train, news_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "630ba07aeecbfba1512b7bcde8788ae129e9ae54"
   },
   "outputs": [],
   "source": [
    "#Make Room\n",
    "del env_market_train\n",
    "del env_news_train\n",
    "del market_train\n",
    "del news_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87c331d7205210c78bf561d091b03e05a2ce4c81"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "219fb704e26034f49bf8b5206257153b7d45268f"
   },
   "source": [
    "# Time To Train\n",
    "# `Featurizer` class\n",
    "\n",
    "Packages Training Data and Test Data Functions for the LSTM:\n",
    "Initializes with :\n",
    "* assetId = the column by which to group assets - string\n",
    "* n_lag = number of days lag - list of ints\n",
    "* shift_size = the standard shift to create a full window length (default = 1)\n",
    "* \"return_features\" Selected Lagged features - list of strings\n",
    "\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdc6b9842073bcb7d63cddc30e5bd7826ccfdfa1"
   },
   "outputs": [],
   "source": [
    "class Featurizer():\n",
    "    def __init__(self, assetId='assetCode',\n",
    "                       n_lag=[3,7,14],\n",
    "                       shift_size=1, \n",
    "                       return_features=['returnsClosePrevMktres10','returnsClosePrevRaw10',\n",
    "                                        'returnsOpenPrevMktres1', 'returnsOpenPrevRaw1',\n",
    "                                        'open','close']\n",
    "                ):\n",
    "        self.assetId = assetId\n",
    "        self.n_lag = n_lag\n",
    "        self.shift_size = shift_size\n",
    "        self.return_features = return_features\n",
    "\n",
    "    def transform(self, df):\n",
    "        new_df = self.generate_lag_features(df)\n",
    "        df = pd.merge(df, new_df, how='left', on=['time', self.assetId])\n",
    "        df = self.mis_impute(df)\n",
    "        return df\n",
    "    \n",
    "    def create_lag(self, df_code):\n",
    "        for col in self.return_features:\n",
    "            for window in self.n_lag:\n",
    "                rolled = df_code[col].shift(self.shift_size).rolling(window=window)\n",
    "                lag_mean = rolled.mean()\n",
    "                lag_max = rolled.max()\n",
    "                lag_min = rolled.min()\n",
    "                lag_std = rolled.std()\n",
    "                df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n",
    "                df_code['%s_lag_%s_max'%(col,window)] = lag_max\n",
    "                df_code['%s_lag_%s_min'%(col,window)] = lag_min\n",
    "                # df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "        return df_code.fillna(-1)\n",
    "\n",
    "    def generate_lag_features(self,df):\n",
    "        features = ['time', self.assetId, 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "        df = df.loc[:,features]\n",
    "    \n",
    "        assetCodes = df[self.assetId].unique()\n",
    "        df_codes = df.groupby(self.assetId)\n",
    "        df_codes = [df_code[1][['time', self.assetId]+self.return_features] for df_code in df_codes]\n",
    "        pool = Pool(4)\n",
    "        all_df = pool.map(self.create_lag, df_codes)\n",
    "        new_df = pd.concat(all_df)  \n",
    "        new_df.drop(self.return_features,axis=1,inplace=True)\n",
    "        pool.close()\n",
    "\n",
    "        return new_df\n",
    "        \n",
    "    def mis_impute(self, df):\n",
    "        for i in df.columns:\n",
    "            if df[i].dtype == \"object\":\n",
    "                df[i] = df[i].fillna(\"other\")\n",
    "            elif (df[i].dtype == \"int32\" or df[i].dtype == \"float32\"):\n",
    "                df[i] = df[i].fillna(df[i].mean())\n",
    "            elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n",
    "                df[i] = df[i].fillna(df[i].mean())\n",
    "            else:\n",
    "                pass\n",
    "        return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c156896349a80d9610481b2d6619aacbcfdf28f"
   },
   "outputs": [],
   "source": [
    "# class AssetEncoder():\n",
    "#     def __init__(self):\n",
    "#         self.assetId = 'assetCode'\n",
    "#         self.le = LabelEncoder()\n",
    "\n",
    "#     def fit(self, X):\n",
    "#         labels = X[self.assetId].values\n",
    "#         self.labels = np.append(labels, 'unknown')\n",
    "#         self.encoder = dict(zip(self.labels,np.arange(len(self.labels))))\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X[self.assetId] = X[self.assetId].apply(self._map_known_labels)\n",
    "#         X['assetCodeT'] = X[self.assetId].apply(self._map_known_labels)\n",
    "#         return X\n",
    "\n",
    "#     def inverse_transform(self, X):\n",
    "#         X[self.assetId] = self.le.inverse_transform(X[self.assetId])\n",
    "#         return X\n",
    "\n",
    "#     def _map_known_labels(self,x):\n",
    "#         if x in self.labels:\n",
    "#             return x\n",
    "#         else:\n",
    "#             return 'unknown'\n",
    "        \n",
    "#     def _encode(self,x):\n",
    "#         return self.encoder[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd1cf2a0ae7282c3862d4c2b0131be4ad55fcb69"
   },
   "outputs": [],
   "source": [
    "# make X, y\n",
    "target = train_df.pop('returnsOpenNextMktres10').values\n",
    "X = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61e8fa1cca96999c83cae4fa25397d64aed4fbe6"
   },
   "outputs": [],
   "source": [
    "#Make a shorter version of our training set. I'll append new observation\n",
    "#data here as it comes in\n",
    "pred_df = train_df.loc[train_df.time>20161201].copy()\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f63a2550fd849ea00a00c63ac1ece61af85ec4c1"
   },
   "outputs": [],
   "source": [
    "#Make lag features\n",
    "featurizer = Featurizer()\n",
    "X = featurizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6230cc10776f1b397a81a7c19fb2ecc8ae077a6b"
   },
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if is_numeric_dtype(col_type):\n",
    "            col_type = col_type.name\n",
    "            \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aeec9d270c8352b98849e7277e7a9ada0f3421c8"
   },
   "outputs": [],
   "source": [
    "X = reduce_mem_usage(X)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16cee611d033210d5fb6512d5215dc62022c8002"
   },
   "outputs": [],
   "source": [
    "drop_cols = ['assetCode','assetName','marketCommentary', 'time']\n",
    "X_features = [c for c in X.columns.values if c not in drop_cols]\n",
    "X = X.loc[:,X_features]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b5799b53f4c41c10ff694f6c82293af1c26c9c1"
   },
   "outputs": [],
   "source": [
    "# Scaling of X values\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9523baf03726eaf429b049ce447a54408f78e248"
   },
   "outputs": [],
   "source": [
    "# Get Train,Val set. Using a random split to expose classifier to different regimes.\n",
    "# Should probably use sklearn time-series K-fold instead, but it performs better on unseen data\n",
    "X_train, X_val, target_train, target_val = train_test_split(X, target, random_state=0)\n",
    "del X, target\n",
    "gc.collect()\n",
    "#make binary y\n",
    "y_train = np.where(target_train>0, 1, 0).astype(int)\n",
    "y_val = np.where(target_val>0, 1, 0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "644ec9018d3dba902d92b631b449b92990a21095"
   },
   "source": [
    "## `BayesianOptimizerLGBM` class\n",
    "\n",
    " Recommended to run this locally since it takes a lot of resources and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a61a4e1aa688257ac762254bd8dbd74b3ef3b37"
   },
   "outputs": [],
   "source": [
    "# class BayesianOptimizerLGBM():\n",
    "#     def __init__(self,spaces):\n",
    "#         self.spaces = spaces\n",
    "\n",
    "#     def fit(self, X_train, y_train, X_val, y_val):\n",
    "#         self.X_train = X_train\n",
    "#         self.y_train = y_train\n",
    "#         self.X_val = X_val\n",
    "#         self.y_val = y_val \n",
    "#         self.res = gp_minimize(self._optimize, self.spaces, acq_func=\"EI\",n_calls=30)\n",
    "\n",
    "#     def _optimize(self, x):\n",
    "\n",
    "#         gbm = LGBMClassifier(learning_rate=x[0],\n",
    "#                             num_leaves=x[1],\n",
    "#                             min_data_in_leaf=x[2],\n",
    "#                             num_iteration=x[3],\n",
    "#                             max_bin=x[4],\n",
    "#                             verbose=1, \n",
    "#                             n_jobs=-1)\n",
    "#         gbm.fit(self.X_train, self.y_train, eval_set=(self.X_val, self.y_val),\n",
    "#                 eval_metric=['binary_logloss'], verbose=True, early_stopping_rounds=5)\n",
    "#         y_pred = gbm.predict_proba(self.X_val)\n",
    "#         score = log_loss(self.y_val, y_pred)\n",
    "#         print(\"score\" , score)\n",
    "#         return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "465f900b1159f70fd8d843d348ae605c35062a25"
   },
   "outputs": [],
   "source": [
    "# spaces = [\n",
    "# (0.10, 0.25), #learning_rate\n",
    "# (1000, 10000), #num_leaves\n",
    "# (200, 400), #min_data_in_leaf\n",
    "# (300, 500), #num_iteration\n",
    "# (200, 400) #max_bin\n",
    "# ]\n",
    "\n",
    "# opt = BayesianOptimizerLGBM(spaces)\n",
    "# opt.fit(X_train, y_train, X_val, y_val)\n",
    "# print(\"optimal params\", opt.res.x)\n",
    "# params = opt.res.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2f8ccf78b99637499142ec401b79e07bcd7ca3c"
   },
   "source": [
    "## `build_model` function\n",
    "\n",
    " Builds my LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60a309803922d4e27dcbe7214711005e50f4945d"
   },
   "outputs": [],
   "source": [
    "def build_lgbm(X_train, y_train, X_val, y_val, boosting_type, params):\n",
    "    gbm = LGBMClassifier(boosting_type=boosting_type,\n",
    "                         learning_rate=params[0],\n",
    "                        num_leaves=params[1],\n",
    "                        min_data_in_leaf=params[2],\n",
    "                        num_iteration=params[3],\n",
    "                        max_bin=params[4],\n",
    "                        verbose=1, \n",
    "                        n_jobs=-1)\n",
    "    gbm.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "            eval_metric=['binary_logloss'], verbose=True, early_stopping_rounds=10)\n",
    "    return gbm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "baf9e437f093db62b066b1d567ef50e010800a4a"
   },
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f97e432c57889c1026a6a21c6f97b9b18cc3bef1"
   },
   "outputs": [],
   "source": [
    "#Params from local skopt gp_minimize\n",
    "params_1 = [0.10192437737356348, 1011, 399, 500, 242]\n",
    "params_2 = [0.14975024553335256, 279, 388, 300, 394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2121f49900e294c6a70e679d64b9d19e2b471835"
   },
   "outputs": [],
   "source": [
    "model_1 = build_lgbm(X_train, y_train, X_val, y_val, 'gbdt', params_1)\n",
    "# model_2 = build_lgbm(X_train, y_train, X_val, y_val, 'dart', params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fc1017497f6e16ff0c10101b4c2937420ba2883"
   },
   "outputs": [],
   "source": [
    "#Confidence Test\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "# preds = model.predict_proba(X_val)[:,1]\n",
    "# confidence_test = (preds-preds.min())/(preds.max()-preds.min())\n",
    "# confidence_test = confidence_test*2-1\n",
    "# print(max(confidence_test),min(confidence_test))\n",
    "\n",
    "# print(minmax.fit_transform(preds)*2-1, confidence_test)\n",
    "\n",
    "# # calculation of actual metric that is used to calculate final score\n",
    "# r_test = target_val.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "# x_t_i = confidence_test * r_test * u_test\n",
    "# data = {'day' : d_test, 'x_t_i' : x_t_i}\n",
    "# df = pd.DataFrame(data)\n",
    "# x_t = df.groupby('day').sum().values.flatten()\n",
    "# mean = np.mean(x_t)\n",
    "# std = np.std(x_t)\n",
    "# score_test = mean / std\n",
    "# print(score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "840aa03b49d675953f080e4069f79f435282bb43"
   },
   "source": [
    "## `get_prediction_days` function (Built-In)\n",
    "\n",
    "Generator which loops through each \"prediction day\" (trading day) and provides all market and news observations which occurred since the last data you've received.  Once you call **`predict`** to make your future predictions, you can continue on to the next prediction day.\n",
    "\n",
    "Yields:\n",
    "* While there are more prediction day(s) and `predict` was called successfully since the last yield, yields a tuple of:\n",
    "    * `market_observations_df`: DataFrame with market observations for the next prediction day.\n",
    "    * `news_observations_df`: DataFrame with news observations for the next prediction day.\n",
    "    * `predictions_template_df`: DataFrame with `assetCode` and `confidenceValue` columns, prefilled with `confidenceValue = 0`, to be filled in and passed back to the `predict` function.\n",
    "* If `predict` has not been called since the last yield, yields `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da"
   },
   "outputs": [],
   "source": [
    "# You can only iterate through a result from `get_prediction_days()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85c026d793233008508589327845e2cf2ed3ed05"
   },
   "outputs": [],
   "source": [
    "# Make room\n",
    "del X_train\n",
    "del X_val\n",
    "del y_train\n",
    "del y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0654d8f143c1b2a18a131944517827d19af4e5d0"
   },
   "source": [
    "## Main Loop\n",
    "Let's loop through all the days and make our  predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb3fd7c0c2e68667bc4521d59eb9dc5fe8a4dbc0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    print(f'___________________________________')\n",
    "    print(f'Getting new data and preparing it')\n",
    "    market_obs_df = prepper.prepare_market(market_obs_df)\n",
    "    news_obs_df = prepper.prepare_news(news_obs_df)\n",
    "    pred_day = prepper.merge_data(market_obs_df, news_obs_df)\n",
    "    print(f'Got it. ready to make some predictions')\n",
    "    date = pred_day.time.values[0]\n",
    "    \n",
    "    #make a historical df\n",
    "    pred_df = pred_df.append(pred_day, sort=True)\n",
    "    lag_date = date_to_int(int_to_date(date) - timedelta(days=30))\n",
    "    short_pred_df = pred_df.loc[pred_df.time >= lag_date]\n",
    "    \n",
    "    #Get historical features to predict on:\n",
    "    print(f'Getting features for the new day {date}')\n",
    "    short_pred_df = featurizer.transform(short_pred_df)\n",
    "    \n",
    "    #make the X Matrix\n",
    "    print(\"setting X for sucess\")\n",
    "    short_pred_df = short_pred_df.loc[short_pred_df.time == date,:]\n",
    "    ordered_df = pd.merge(predictions_template_df, short_pred_df, how='left', on='assetCode')\n",
    "    X = ordered_df.loc[:, X_features]\n",
    "    \n",
    "\n",
    "    print(\"making predictions\")\n",
    "    raw_pred = model_1.predict_proba(X)\n",
    "    pred_scaled = minmax.fit_transform(raw_pred)[:,1]\n",
    "    mod_pred = pred_scaled*2-1\n",
    "    predictions_template_df.confidenceValue = np.clip(mod_pred, -0.99, 0.99)\n",
    "    print(\"Submitting predictions!\")\n",
    "    env.predict(predictions_template_df)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba72731adf652d6011652e906d8b340d6572904e"
   },
   "source": [
    "### **`predict`** function (Built-In)\n",
    "Stores your predictions for the current prediction day.  Expects the same format as you saw in `predictions_template_df` returned from `get_prediction_days`.\n",
    "\n",
    "Args:\n",
    "* `predictions_df`: DataFrame which must have the following columns:\n",
    "    * `assetCode`: The market asset.\n",
    "    * `confidenceValue`: Your confidence whether the asset will increase or decrease in 10 trading days.  All values must be in the range `[-1.0, 1.0]`.\n",
    "\n",
    "The `predictions_df` you send **must** contain the exact set of rows which were given to you in the `predictions_template_df` returned from `get_prediction_days`.  The `predict` function does not validate this, but if you are missing any `assetCode`s or add any extraneous `assetCode`s, then your submission will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "## **`write_submission_file`** function (Built-in)\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
