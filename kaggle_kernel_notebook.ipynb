{
  "cells": [
    {
      "metadata": {
        "_uuid": "b4877d6f4090732087a6ecb9b721a5331bb3f488"
      },
      "cell_type": "markdown",
      "source": "# ESKEEETIT"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ea12a1be88e733b70a135265d93ae517bb7f0c0"
      },
      "cell_type": "code",
      "source": "##Import all the things\n#Triple Threat---\nimport gc\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None #shut down that annoying slice warning\nfrom pandas.api.types import is_numeric_dtype\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\n#Bayesian Optimization\n# from skopt import gp_minimize\n\n#You know it, you love it\nfrom lightgbm import LGBMClassifier\n\n#For featurizing\nfrom multiprocessing import Pool\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69b0a67c1b6881ff1ee3996dac6195a0adf4f3e5"
      },
      "cell_type": "code",
      "source": "#Datetime functions bc I committed early to integer datetime and I don't want to refactor\ndef date_to_int(dt_time):\n    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\ndef int_to_date(x):\n    return datetime.strptime(str(date), '%Y%m%d')\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Get the training data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6034b46fce8c9d55d403de32e7cebe8cb9fef96d"
      },
      "cell_type": "markdown",
      "source": "## **`get_training_data`** function (built-in)\n\nReturns the training data DataFrames as a tuple of:\n* `market_train_df`: DataFrame with market training data\n* `news_train_df`: DataFrame with news training data\n\nThese DataFrames contain all market and news data from February 2007 to December 2016. "
    },
    {
      "metadata": {
        "_uuid": "5cffd5e960d4c129d7b12638af26125ce3d66199"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
      },
      "cell_type": "code",
      "source": "(env_market_train, env_news_train) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ac338c0f69be1f26754e49515e4a72bf8e8f383"
      },
      "cell_type": "markdown",
      "source": "## **`DataPrepper`** class\nTakes environment dataframes:\n* env_market_train\n* env_news_train\n\nPerforms data cleaning:\n* Replace Price Outliers\n* Convert Datetime\n* Remove data prior to train cutoff\n\nPerforms feature engineering:\n* Create Daily Return\n* Daily Change\n* Price/Volume\n* All News Data aggregated  to relevant trading day\n\nReturns merged dataframe for training\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6811a1a76f08b2a029543cf73bcdf4dfca7dc362"
      },
      "cell_type": "code",
      "source": "class DataPrepper():\n    \"\"\"A class for loading and transforming data for stock data\"\"\"\n\n    def __init__(self):\n        self.train_cutoff = 20101231\n\n    def make_price_diff(self, market_train):\n        #eda function to find outliers\n        market_train['closeOverOpen'] = market_train['Close']/market_train['Open']\n        market_train['priceDiff'] = np.abs(market_train['Close'] - market_train['Open'])\n        return market_train\n\n    def _replace_price_outliers(self, market_train):\n        market_train['dailychange'] = market_train['close']/market_train['open']\n        market_train['open'][market_train['dailychange'] < .33] = market_train['close']\n        market_train['close'][market_train['dailychange'] > 2] = market_train['open']\n        return market_train\n\n    def prepare_market(self, market_train):\n        market_train = self._replace_price_outliers(market_train)\n        market_train['time'] = market_train['time'].dt.strftime(\"%Y%m%d\").astype(int)\n        market_train = market_train[market_train.time >= self.train_cutoff]\n        market_train['pricevolume'] = market_train['volume']/market_train['close']\n        self.tradingdays = market_train['time'].unique()\n        return market_train\n\n    def prepare_news(self, news_train):\n        news_train['time'] = news_train['time'].dt.strftime(\"%Y%m%d\").astype(int)\n        news_train = news_train[news_train.time >= self.train_cutoff]\n        news_train['time'] = news_train['time'].apply(self._map_trading_day)\n        news_train['coverage'] = news_train['sentimentWordCount']/news_train['wordCount']\n        return news_train\n\n    def _map_trading_day(self, news_date):\n        if news_date in self.tradingdays:\n            return news_date\n        else:   \n            values = self.tradingdays - news_date\n            mask = values >= 0\n            try:\n                return self.tradingdays[mask][0]\n            except:\n                return 0\n\n    def merge_data(self, market_df, news_df):\n        newsgroup = news_df.groupby(['time', 'assetName'], sort=False).agg(np.mean).reset_index()\n        merged = pd.merge(market_df, newsgroup, how='left', on=['time', 'assetName'], copy=False)\n        merged.fillna(value=0, inplace=True)\n        return merged",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84b5a58f67ebded82e6aabc66ca36411e6db35a9",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#Prepare the market and news data\nprepper = DataPrepper()\nmarket_train = prepper.prepare_market(env_market_train)\nnews_train = prepper.prepare_news(env_news_train)\ntrain_df = prepper.merge_data(market_train, news_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "630ba07aeecbfba1512b7bcde8788ae129e9ae54"
      },
      "cell_type": "code",
      "source": "#Make Room\ndel env_market_train\ndel env_news_train\ndel market_train\ndel news_train\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87c331d7205210c78bf561d091b03e05a2ce4c81"
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "219fb704e26034f49bf8b5206257153b7d45268f"
      },
      "cell_type": "markdown",
      "source": "# Time To Train\n# `Featurizer` class\n\nPackages Training Data and Test Data Functions for the LSTM:\nInitializes with :\n* assetId = the column by which to group assets - string\n* n_lag = number of days lag - list of ints\n* shift_size = the standard shift to create a full window length (default = 1)\n* \"return_features\" Selected Lagged features - list of strings\n\n\n**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdc6b9842073bcb7d63cddc30e5bd7826ccfdfa1"
      },
      "cell_type": "code",
      "source": "class Featurizer():\n    def __init__(self, assetId='assetCode',\n                       n_lag=[3,7,14],\n                       shift_size=1, \n                       return_features=['returnsClosePrevMktres10','returnsClosePrevRaw10',\n                                        'returnsOpenPrevMktres1', 'returnsOpenPrevRaw1',\n                                        'open','close']\n                ):\n        self.assetId = assetId\n        self.n_lag = n_lag\n        self.shift_size = shift_size\n        self.return_features = return_features\n\n    def transform(self, df):\n        new_df = self.generate_lag_features(df)\n        df = pd.merge(df, new_df, how='left', on=['time', self.assetId])\n        df = self.mis_impute(df)\n        return df\n    \n    def create_lag(self, df_code):\n        for col in self.return_features:\n            for window in self.n_lag:\n                rolled = df_code[col].shift(self.shift_size).rolling(window=window)\n                lag_mean = rolled.mean()\n                lag_max = rolled.max()\n                lag_min = rolled.min()\n                lag_std = rolled.std()\n                df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n                df_code['%s_lag_%s_max'%(col,window)] = lag_max\n                df_code['%s_lag_%s_min'%(col,window)] = lag_min\n                # df_code['%s_lag_%s_std'%(col,window)] = lag_std\n        return df_code.fillna(-1)\n\n    def generate_lag_features(self,df):\n        features = ['time', self.assetId, 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        df = df.loc[:,features]\n    \n        assetCodes = df[self.assetId].unique()\n        df_codes = df.groupby(self.assetId)\n        df_codes = [df_code[1][['time', self.assetId]+self.return_features] for df_code in df_codes]\n        pool = Pool(4)\n        all_df = pool.map(self.create_lag, df_codes)\n        new_df = pd.concat(all_df)  \n        new_df.drop(self.return_features,axis=1,inplace=True)\n        pool.close()\n\n        return new_df\n        \n    def mis_impute(self, df):\n        for i in df.columns:\n            if df[i].dtype == \"object\":\n                df[i] = df[i].fillna(\"other\")\n            elif (df[i].dtype == \"int32\" or df[i].dtype == \"float32\"):\n                df[i] = df[i].fillna(df[i].mean())\n            elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n                df[i] = df[i].fillna(df[i].mean())\n            else:\n                pass\n        return df\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c156896349a80d9610481b2d6619aacbcfdf28f"
      },
      "cell_type": "code",
      "source": "# class AssetEncoder():\n#     def __init__(self):\n#         self.assetId = 'assetCode'\n#         self.le = LabelEncoder()\n\n#     def fit(self, X):\n#         labels = X[self.assetId].values\n#         self.labels = np.append(labels, 'unknown')\n#         self.encoder = dict(zip(self.labels,np.arange(len(self.labels))))\n\n#     def transform(self, X):\n#         X[self.assetId] = X[self.assetId].apply(self._map_known_labels)\n#         X['assetCodeT'] = X[self.assetId].apply(self._map_known_labels)\n#         return X\n\n#     def inverse_transform(self, X):\n#         X[self.assetId] = self.le.inverse_transform(X[self.assetId])\n#         return X\n\n#     def _map_known_labels(self,x):\n#         if x in self.labels:\n#             return x\n#         else:\n#             return 'unknown'\n        \n#     def _encode(self,x):\n#         return self.encoder[x]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd1cf2a0ae7282c3862d4c2b0131be4ad55fcb69"
      },
      "cell_type": "code",
      "source": "# make X, y\ntarget = train_df.pop('returnsOpenNextMktres10').values\nX = train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61e8fa1cca96999c83cae4fa25397d64aed4fbe6"
      },
      "cell_type": "code",
      "source": "#Make a shorter version of our training set. I'll append new observation\n#data here as it comes in\npred_df = train_df.loc[train_df.time>20161201].copy()\ndel train_df\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f63a2550fd849ea00a00c63ac1ece61af85ec4c1"
      },
      "cell_type": "code",
      "source": "#Make lag features\nfeaturizer = Featurizer()\nX = featurizer.transform(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6230cc10776f1b397a81a7c19fb2ecc8ae077a6b"
      },
      "cell_type": "code",
      "source": "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if is_numeric_dtype(col_type):\n            col_type = col_type.name\n            \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aeec9d270c8352b98849e7277e7a9ada0f3421c8"
      },
      "cell_type": "code",
      "source": "X = reduce_mem_usage(X)\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16cee611d033210d5fb6512d5215dc62022c8002"
      },
      "cell_type": "code",
      "source": "drop_cols = ['assetCode','assetName','marketCommentary', 'time']\nX_features = [c for c in X.columns.values if c not in drop_cols]\nX = X.loc[:,X_features]\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b5799b53f4c41c10ff694f6c82293af1c26c9c1"
      },
      "cell_type": "code",
      "source": "# Scaling of X values\n\n# scaler = StandardScaler()\n# scaler.fit(X)\n# X = scaler.transform(X)\n# gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9523baf03726eaf429b049ce447a54408f78e248"
      },
      "cell_type": "code",
      "source": "# Get Train,Val set. Using a random split to expose classifier to different regimes.\n# Should probably use sklearn time-series K-fold instead, but it performs better on unseen data\nX_train, X_val, target_train, target_val = train_test_split(X, target, random_state=0)\ndel X, target\ngc.collect()\n#make binary y\ny_train = np.where(target_train>0, 1, 0).astype(int)\ny_val = np.where(target_val>0, 1, 0).astype(int)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "644ec9018d3dba902d92b631b449b92990a21095"
      },
      "cell_type": "markdown",
      "source": "## `BayesianOptimizerLGBM` class\n\n Recommended to run this locally since it takes a lot of resources and time."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a61a4e1aa688257ac762254bd8dbd74b3ef3b37"
      },
      "cell_type": "code",
      "source": "# class BayesianOptimizerLGBM():\n#     def __init__(self,spaces):\n#         self.spaces = spaces\n\n#     def fit(self, X_train, y_train, X_val, y_val):\n#         self.X_train = X_train\n#         self.y_train = y_train\n#         self.X_val = X_val\n#         self.y_val = y_val \n#         self.res = gp_minimize(self._optimize, self.spaces, acq_func=\"EI\",n_calls=30)\n\n#     def _optimize(self, x):\n\n#         gbm = LGBMClassifier(learning_rate=x[0],\n#                             num_leaves=x[1],\n#                             min_data_in_leaf=x[2],\n#                             num_iteration=x[3],\n#                             max_bin=x[4],\n#                             verbose=1, \n#                             n_jobs=-1)\n#         gbm.fit(self.X_train, self.y_train, eval_set=(self.X_val, self.y_val),\n#                 eval_metric=['binary_logloss'], verbose=True, early_stopping_rounds=5)\n#         y_pred = gbm.predict_proba(self.X_val)\n#         score = log_loss(self.y_val, y_pred)\n#         print(\"score\" , score)\n#         return score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "465f900b1159f70fd8d843d348ae605c35062a25"
      },
      "cell_type": "code",
      "source": "# spaces = [\n# (0.10, 0.25), #learning_rate\n# (1000, 10000), #num_leaves\n# (200, 400), #min_data_in_leaf\n# (300, 500), #num_iteration\n# (200, 400) #max_bin\n# ]\n\n# opt = BayesianOptimizerLGBM(spaces)\n# opt.fit(X_train, y_train, X_val, y_val)\n# print(\"optimal params\", opt.res.x)\n# params = opt.res.x\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2f8ccf78b99637499142ec401b79e07bcd7ca3c"
      },
      "cell_type": "markdown",
      "source": "## `build_model` function\n\n Builds my LGB Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60a309803922d4e27dcbe7214711005e50f4945d"
      },
      "cell_type": "code",
      "source": "def build_lgbm(X_train, y_train, X_val, y_val, boosting_type, params):\n    gbm = LGBMClassifier(boosting_type=boosting_type,\n                         learning_rate=params[0],\n                        num_leaves=params[1],\n                        min_data_in_leaf=params[2],\n                        num_iteration=params[3],\n                        max_bin=params[4],\n                        verbose=1, \n                        n_jobs=-1)\n    gbm.fit(X_train, y_train, eval_set=(X_val, y_val),\n            eval_metric=['binary_logloss'], verbose=True, early_stopping_rounds=10)\n    return gbm\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "baf9e437f093db62b066b1d567ef50e010800a4a"
      },
      "cell_type": "markdown",
      "source": "## Fit the Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f97e432c57889c1026a6a21c6f97b9b18cc3bef1"
      },
      "cell_type": "code",
      "source": "#Params from local skopt gp_minimize\nparams_1 = [0.10192437737356348, 1011, 399, 500, 242]\nparams_2 = [0.14975024553335256, 279, 388, 300, 394]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2121f49900e294c6a70e679d64b9d19e2b471835"
      },
      "cell_type": "code",
      "source": "model_1 = build_lgbm(X_train, y_train, X_val, y_val, 'gbdt', params_1)\n# model_2 = build_lgbm(X_train, y_train, X_val, y_val, 'dart', params_2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fc1017497f6e16ff0c10101b4c2937420ba2883"
      },
      "cell_type": "code",
      "source": "#Confidence Test\nminmax = MinMaxScaler()\n\n# preds = model.predict_proba(X_val)[:,1]\n# confidence_test = (preds-preds.min())/(preds.max()-preds.min())\n# confidence_test = confidence_test*2-1\n# print(max(confidence_test),min(confidence_test))\n\n# print(minmax.fit_transform(preds)*2-1, confidence_test)\n\n# # calculation of actual metric that is used to calculate final score\n# r_test = target_val.clip(-1,1) # get rid of outliers. Where do they come from??\n# x_t_i = confidence_test * r_test * u_test\n# data = {'day' : d_test, 'x_t_i' : x_t_i}\n# df = pd.DataFrame(data)\n# x_t = df.groupby('day').sum().values.flatten()\n# mean = np.mean(x_t)\n# std = np.std(x_t)\n# score_test = mean / std\n# print(score_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "840aa03b49d675953f080e4069f79f435282bb43"
      },
      "cell_type": "markdown",
      "source": "## `get_prediction_days` function (Built-In)\n\nGenerator which loops through each \"prediction day\" (trading day) and provides all market and news observations which occurred since the last data you've received.  Once you call **`predict`** to make your future predictions, you can continue on to the next prediction day.\n\nYields:\n* While there are more prediction day(s) and `predict` was called successfully since the last yield, yields a tuple of:\n    * `market_observations_df`: DataFrame with market observations for the next prediction day.\n    * `news_observations_df`: DataFrame with news observations for the next prediction day.\n    * `predictions_template_df`: DataFrame with `assetCode` and `confidenceValue` columns, prefilled with `confidenceValue = 0`, to be filled in and passed back to the `predict` function.\n* If `predict` has not been called since the last yield, yields `None`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da"
      },
      "cell_type": "code",
      "source": "# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85c026d793233008508589327845e2cf2ed3ed05"
      },
      "cell_type": "code",
      "source": "# Make room\ndel X_train\ndel X_val\ndel y_train\ndel y_val\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0654d8f143c1b2a18a131944517827d19af4e5d0"
      },
      "cell_type": "markdown",
      "source": "## Main Loop\nLet's loop through all the days and make our  predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "fb3fd7c0c2e68667bc4521d59eb9dc5fe8a4dbc0"
      },
      "cell_type": "code",
      "source": "\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    print(f'___________________________________')\n    print(f'Getting new data and preparing it')\n    market_obs_df = prepper.prepare_market(market_obs_df)\n    news_obs_df = prepper.prepare_news(news_obs_df)\n    pred_day = prepper.merge_data(market_obs_df, news_obs_df)\n    print(f'Got it. ready to make some predictions')\n    date = pred_day.time.values[0]\n    \n    #make a historical df\n    pred_df = pred_df.append(pred_day, sort=True)\n    lag_date = date_to_int(int_to_date(date) - timedelta(days=30))\n    short_pred_df = pred_df.loc[pred_df.time >= lag_date]\n    \n    #Get historical features to predict on:\n    print(f'Getting features for the new day {date}')\n    short_pred_df = featurizer.transform(short_pred_df)\n    \n    #make the X Matrix\n    print(\"setting X for sucess\")\n    short_pred_df = short_pred_df.loc[short_pred_df.time == date,:]\n    ordered_df = pd.merge(predictions_template_df, short_pred_df, how='left', on='assetCode')\n    X = ordered_df.loc[:, X_features]\n    \n\n    print(\"making predictions\")\n    raw_pred = model_1.predict_proba(X)\n    pred_scaled = minmax.fit_transform(raw_pred)[:,1]\n    mod_pred = pred_scaled*2-1\n    predictions_template_df.confidenceValue = np.clip(mod_pred, -0.99, 0.99)\n    print(\"Submitting predictions!\")\n    env.predict(predictions_template_df)\n    gc.collect()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba72731adf652d6011652e906d8b340d6572904e"
      },
      "cell_type": "markdown",
      "source": "### **`predict`** function (Built-In)\nStores your predictions for the current prediction day.  Expects the same format as you saw in `predictions_template_df` returned from `get_prediction_days`.\n\nArgs:\n* `predictions_df`: DataFrame which must have the following columns:\n    * `assetCode`: The market asset.\n    * `confidenceValue`: Your confidence whether the asset will increase or decrease in 10 trading days.  All values must be in the range `[-1.0, 1.0]`.\n\nThe `predictions_df` you send **must** contain the exact set of rows which were given to you in the `predictions_template_df` returned from `get_prediction_days`.  The `predict` function does not validate this, but if you are missing any `assetCode`s or add any extraneous `assetCode`s, then your submission will fail."
    },
    {
      "metadata": {
        "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
      },
      "cell_type": "markdown",
      "source": "## **`write_submission_file`** function (Built-in)\n\nWrites your predictions to a CSV file (`submission.csv`) in the current working directory."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
      },
      "cell_type": "code",
      "source": "env.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
      },
      "cell_type": "code",
      "source": "# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
      },
      "cell_type": "markdown",
      "source": "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}